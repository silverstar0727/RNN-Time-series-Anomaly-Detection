# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ASww4kVaU4wjjwEaDNCzzWts7Vi_hiqS
"""

import numpy as np
import random
import matplotlib.pyplot as plt
import pandas as pd

df = pd.read_csv('sensor.csv')

"""총 6개의 broken data, broken이후 recovering중인 데이터, 정상데이터로 구성
'broken'은 각 타임스탬프에서 이상이 있다는것을 의미하며, 'recovering'은 현재 타임스텝에서 이상이 발생한 직후에 펌프를 이상으로 복구중임을 의미한다.이러한 값을 후에 각각 라벨인코딩(0,1,2)
"""

from matplotlib import pyplot as plt
import seaborn as sns
df.groupby('machine_status').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

df['sensor_00'].isnull().sum()

"""인덱스 제거"""

#index 제거
df.drop(df.columns[0], axis=1, inplace=True)

df.describe()

df.isnull().sum()

"""### periodic
- 비슷한 pattern을 보이는 sensor들이 존재
  - sensor (1,2,3)
  - (4,5,6,7,8,9),
  - (10,11,12),
  - (14,16,17,18),
  - (19,20,21,22,23,24),
  - (25,26,28,29,30,31,32,33),
  - (34,35),
  - (38,39,40,41,42,43,45,46,47).
- 비슷한 추세, pattern을 보이는 sensor들을 모델에다가 다 학습시키는게 나을까/ 아님 pca나 비슷한 pattern을 보이는 sensor column중 일부만 채택하여 모델에 학습시킬지

sensor15 column은 모든 행에 대해서 null값이므로 제거
- 추가 eda방법론으로는 data Null값이 10만개 이상인 컬럼을 제거하고 모델에 집어넣어도 될듯..? -> 이미 비슷한 pattern을 띄는센서컬럼이 많으므로

나머지 null값들은 시계열의 마지막값으로 채움(추가적으로 0값은 누락된 값이 아니라 센서 자체에서 0값을 반환하였다고 판단.)
"""

df.drop(['sensor_15'], axis=1, inplace=True)
df.fillna(method='ffill', inplace=True)

"""recovering : 2, broken : 1, Normal : 0 으로 라벨 인코딩"""

df.replace({'machine_status': {'RECOVERING': 2, 'BROKEN': 1, 'NORMAL': 0}}, inplace=True)
df['timestamp'] = pd.to_datetime(df['timestamp'])

# machine status == 2 or 1 인 경우 test
# machine status == 0 인 경우 train

train = df.loc[df['machine_status'] == 0]
test = df.loc[df['machine_status'] != 0]

import pickle

import numpy as np

import os


os.makedirs('dataset/custom/labeled/train', exist_ok=True)
os.makedirs('dataset/custom/labeled/test', exist_ok=True)

# Convert timestamp to Unix timestamp (numerical format)
train['timestamp'] = pd.to_datetime(train['timestamp']).astype(np.int64) // 10**9  # Convert to seconds
test['timestamp'] = pd.to_datetime(test['timestamp']).astype(np.int64) // 10**9

# Now convert to numpy arrays
train_array = train.values.astype(np.float32)
test_array = test.values.astype(np.float32)

# Save the numpy arrays
with open('dataset/custom/labeled/train/custom.pkl', 'wb') as f:
    pickle.dump(train_array, f)

with open('dataset/custom/labeled/test/custom.pkl', 'wb') as f:
    pickle.dump(test_array, f)